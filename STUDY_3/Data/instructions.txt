1) DOWNLOAD DATA FROM MLAB DB SERVER & IMPORT TO LOCAL DB
Run script to perform download, pass in desired sessionname, upload to local db
//mongoexport -h ds259325.mlab.com:59325 -d 2ypdb-s3-beh -c entries -u expadmin -p thirdyear --out PARAMETER.json
//mongoimport -d STUDY3 -c alfa-bravo-charlie --file PARAMETER.json
>> sh download.sh [sessionName]

2) BACKUP DATA TO GITHUB
cd into data dump folder
>> mongodump -h localhost -d [DB NAME]






----------------------------------------------------------------------------------------------
:: /session_files raw exports (json) from each session to be downloaded to
:: /db_scripts scripts to extract (processed) data from local db to file for analysis import
:: /data_files processed data files for import with R analysis


DESIRED OUTPUT
final_participants.json is a participant level data file
final_items is a question level data file



STEP ONE: EXPORT DATA FROM DB SERVER TO FILE
---
//for each session, export the records into a json file for the session from the server
mongoexport -h [connectionstring] -d [dbname] -c [tablename] -u [username] -p [password] --out [filename]
mongoexport -h ds259325.mlab.com:59325 -d 2ypdb-s3-beh -c entries -u expadmin -p thirdyear --out alfa-bravo-charlie.json


STEP TWO: IMPORT DATA FROM SESSION FILES TO LOCAL DB
---
//start local database (in any directory) run
mongod
//import the previously exported file into the local DB for analysis
mongoimport -d [DBNAME] -c [SESSIONNAME] --file [filename.json]
mongoimport -d STUDY3 -c alfa-bravo-charlie --file alfa-bravo-charlie.json

STEP THREE: EXTRACT DATA FROM LOCAL db
// start (visual) DB GUI (roboMongo)
// access relevant data analysis script (ie. flatten.js)
// add copyTo line for new session data file
// generate final data files for each level of desired analysis
// WRANGLE (with regex) the output .js files to .json format --> grumble
-------------------------------------------------------------
REGEX FOR WRANGLING JSON FILES
/* x */ == \/\* .+ \*\/ (replace with comma)
[objectID line]        =="_id" : ([A-Z])\w+\("\w+"\),
-------------------------------------------------------------
// save the data files in data_files, and also in the /data folder of the Analysis project

levels of analysis
:: participant
:: item
:: mouse

-------------------------------------------------------------
REGEX FOR WRANGLING JSON FILES
/* x */ == \/\* .+ \*\/ (replace with comma) //EXCEPT THE FIRST LINE, MANUALLY REMOVE THAT COMMA
[objectID line]        =="_id" : ([A-Z])\w+\("\w+"\),
-------------------------------------------------------------

STEP FOUR: manually construct master spss / R data sets
> in each _participants.xls file created, manually split demographics fields into separate columns by ','delimiter
> create _min (or _m) translations for each time field (recorded in miliseconds)

DEALING WITH MOUSEFLOW DATA
Free and starter (39$) accounts do not include API access, or any way to download the .zip files of the recordings in bulk.
To work around this issue, login to mouseflow and download the csv file of the recordings which includes their unique IDs and metadata.
extract the recording unique IDs and edit in atom, so that you have a URL matching this pattern for each file
https://us.mouseflow.com/websites/e0c9ff05-6fcb-4c4a-8173-e5a0724987b1/recordings/RECORDINGIDHERE/download
Using the chrome extension Open Multiple URLs 1.3.2.1,
have chrome open a tab for each URL on the list, which automatically starts the download.
(recommend "Don't load tabs until selected" option, and only loading 100-500 at a time so that chrome doesn't crash)
